{
  "hash": "b5a57771d45fd951c1e23d886c8c00ad",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: TF-IDF on Trump Tweets\ndate: \"12-07-2024\"\njupyter: python3\n---\n\n\nI made a model to predictively flag Trump's post-2021 tweets based off the flagged or non-flagged statuses of his pre-2021 tweets. While the model did not have the best results, I learned a lot about how to do text-based analysis!\n\nTo understand the topic, here's a video I made explaining how TF-IDF works!\n\n{{< video media1/TFIDFTeaching.mp4 >}}\n\n\n\n## Importing Modules, Reading Dataset, Indexing Dataset\n\n::: {#9d02a3aa .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\npre_2021 = pd.read_csv('media1/tweets_01-08-2021.csv')\n#print(pre_2021.text)\n\n#two dataframes\nindex = pre_2021['isFlagged'] == 't'\nflagged_pre_2021 = pre_2021[index]\nn_flagged_pre_2021 = pre_2021[~index]\n```\n:::\n\n\n## Running TF-IDF on the Flagged Data\n\n::: {#b499d5a9 .cell execution_count=2}\n``` {.python .cell-code}\n#tfidf isFlagged\nfrom sklearn.feature_extraction.text import TfidfVectorizer #scikitlearn library, feature_extraction module, TfidfVectorizer Class \n\ncustom_stop_words = [\"t.co\", \"https\", \"http\", \"rt\", \"amp\", \"www\", \"co\", \"com\"]\ndefault_stop_words = TfidfVectorizer(stop_words='english').get_stop_words()\ncombined_stop_words = list(default_stop_words.union(custom_stop_words))\n\n# Combine custom stop words with the default English stop words\ntfidf = TfidfVectorizer(\n    stop_words=combined_stop_words, \n    max_features=3000,\n    ngram_range = (1,3)\n)\n\n#Fit the TfidfVectorizer to the data\nresult = tfidf.fit_transform(flagged_pre_2021.text)\nkeys = list(tfidf.vocabulary_.keys())\n#print(keys)\n\n# # get indexing for first 30 values\nprint('\\nWord indexes:')\nfirst_few = list(tfidf.vocabulary_.items())[:30]\nfor key, value in first_few:\n    print(f\"{key}: {value}\")\n#print(tfidf.vocabulary_)\n \n# # display tf-idf values\nprint('\\ntf-idf value:')\nprint(result)\n\n#check tfidf matrix dimensions\n#number of documents by number of features\nprint(result.toarray().shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nWord indexes:\nnevada: 2598\ncesspool: 445\nfake: 1622\nvotes: 2912\nfinding: 1698\nthings: 2828\ncesspool fake: 446\nfake votes: 1648\nfinding things: 1703\ncesspool fake votes: 447\nfake votes mschlapp: 1649\nfinding things released: 1704\npennsylvania: 2629\nwatching: 2944\nballot: 77\ncount: 739\nillegal: 2395\ncountry: 799\nballot count: 78\ncount unthinkable: 751\nillegal country: 2398\ncount unthinkable illegal: 752\nteamtrump: 2817\nlive: 2535\npress: 2677\nconference: 656\nchairwoman: 448\nteamtrump live: 2818\nconference kayleigh: 659\nchairwoman ronna: 451\n\ntf-idf value:\n<Compressed Sparse Row sparse matrix of dtype 'float64'\n\twith 5681 stored elements and shape (304, 3000)>\n  Coords\tValues\n  (0, 2598)\t0.24973806396072906\n  (0, 445)\t0.3243383980144909\n  (0, 1622)\t0.1953010208212467\n  (0, 2912)\t0.15560878149712018\n  (0, 1698)\t0.28703823098760994\n  (0, 2828)\t0.23773009225078487\n  (0, 446)\t0.3243383980144909\n  (0, 1648)\t0.3243383980144909\n  (0, 1703)\t0.3243383980144909\n  (0, 447)\t0.3243383980144909\n  (0, 1649)\t0.3243383980144909\n  (0, 1704)\t0.3243383980144909\n  (1, 2629)\t0.21824894321863983\n  (1, 2944)\t0.3042825942418986\n  (1, 77)\t0.23426573698111625\n  (1, 739)\t0.28826580047942213\n  (1, 2395)\t0.29577235950880687\n  (1, 799)\t0.2515966410114136\n  (1, 78)\t0.3582826577402044\n  (1, 751)\t0.38412379650359346\n  (1, 2398)\t0.38412379650359346\n  (1, 752)\t0.38412379650359346\n  (2, 2817)\t0.28395367876274485\n  (2, 2535)\t0.28395367876274485\n  (2, 2677)\t0.3123345034376439\n  :\t:\n  (303, 2496)\t0.1327285528588964\n  (303, 2856)\t0.12005888852084247\n  (303, 2102)\t0.14298870514984965\n  (303, 2539)\t0.14011030504238925\n  (303, 2559)\t0.12490863886921652\n  (303, 2703)\t0.17645320454777802\n  (303, 2977)\t0.16304012167139642\n  (303, 2562)\t0.146141635735278\n  (303, 2733)\t0.16907145236428517\n  (303, 2840)\t0.16907145236428517\n  (303, 2866)\t0.16304012167139642\n  (303, 1925)\t0.1859699383004036\n  (303, 2498)\t0.1859699383004036\n  (303, 489)\t0.3987660423535704\n  (303, 966)\t0.1993830211767852\n  (303, 2109)\t0.1993830211767852\n  (303, 490)\t0.1993830211767852\n  (303, 967)\t0.1993830211767852\n  (303, 1846)\t0.1993830211767852\n  (303, 1928)\t0.1993830211767852\n  (303, 2110)\t0.1993830211767852\n  (303, 491)\t0.1993830211767852\n  (303, 968)\t0.1993830211767852\n  (303, 1847)\t0.1993830211767852\n  (303, 1929)\t0.1993830211767852\n(304, 3000)\n```\n:::\n:::\n\n\nWhen looking at the coordinate, the left coordinate is the document, or in our case tweet, that the word/phrase is in. The right coordinate is a word/phrase which corresponds to the tfidf vocabulary_ dictionary. Finally the values column gives us the TF-IDF scores/values: the higher it is, the more important it is!\n\n## Running TF-IDF on the Non-Flagged Data\n\n::: {#8520e0ff .cell execution_count=3}\n``` {.python .cell-code}\n#tfidf is not Flagged\n#we add these custom ones to remove links that Trump posts\ncustom_stop_words = [\"t.co\", \"https\", \"http\", \"rt\", \"amp\", \"www\", \"co\", \"com\"]\ndefault_stop_words = TfidfVectorizer(stop_words='english').get_stop_words()\ncombined_stop_words = list(default_stop_words.union(custom_stop_words))\n\n# Combine custom stop words with the default English stop words\ntfidf2 = TfidfVectorizer(\n    stop_words=combined_stop_words,  \n    max_features=3000, #3000 features to look for\n    ngram_range = (1,3) #each feature can be 1 to 3 words long (to capture nuance)\n)\n\n# Fit the TfidfVectorizer to the data\nresult2 = tfidf2.fit_transform(n_flagged_pre_2021.text)\n\n#the key features\nkeys2 = list(tfidf2.vocabulary_.keys())\n#print(keys2)\n\n# # get indexing\n# print('\\nWord indexes:')\n# print(tfidf.vocabulary_)\n \n# # display tf-idf values\n# print('\\ntf-idf value:')\n# print(result)\n```\n:::\n\n\n## Taking the difference between the two word sets\n\n::: {#d389e1f9 .cell execution_count=4}\n``` {.python .cell-code}\n#difference between vocab sets\nkeys_set = set(keys)\nkeys2_set = set(keys2)\ndifference_keys = keys_set.difference(keys2_set)\n#print(difference_keys)\n```\n:::\n\n\n## Quick visualization of word overlap\n\n::: {#75998986 .cell execution_count=5}\n``` {.python .cell-code}\nfrom matplotlib_venn import venn2\nimport matplotlib.pyplot as plt\n\n# Create sets from your word lists\nflagged_set = keys_set  # Words in the flagged tweets\nnon_flagged_set = keys2_set  # Words in non-flagged tweets\n\n# Create a Venn diagram\nvenn = venn2([flagged_set, non_flagged_set], set_labels=('Flagged Set', 'Non-Flagged Set'))\nvenn.get_patch_by_id('10').set_color('lightgreen')  # Color for flagged set\nvenn.get_patch_by_id('01').set_color('lightgray')  # Color for non-flagged set\nvenn.get_patch_by_id('11').set_color('red')  # Color for intersection\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](01-project_files/figure-html/cell-6-output-1.png){width=540 height=351}\n:::\n:::\n\n\nThe values we will use to predict if a tweet is flagged are the ones in green.\n\nI take the dictionary \"difference\" that I just found which is the words in the isFlagged tweets but not the non-flagged ones and then place that set into a function \"tweet_flagger\". This essentially searches through all the post-2021 tweets and changes their flag_prediction value to True if it detects any of the words in the dictionary.\n\n::: {#cd060e9c .cell execution_count=6}\n``` {.python .cell-code}\nimport string\n\ndef tweet_flagger(df):\n    flagged_words = difference_keys\n    for index, row in df.iterrows():\n        tweet = row['text']  # Replace 'tweet' with the actual column name that holds the tweet text\n        tweet = tweet.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n        separated_words = tweet.split()  # Split the tweet into words\n\n        # Check if any word in the tweet is in the flagged dictionary\n        if any(word.lower() in flagged_words for word in separated_words):\n            df.at[index, 'flag_prediction'] = 't'  # Update flag_prediction to 't'\n    return df[['text', 'flag_prediction']]    \n```\n:::\n\n\n## Running the function on the pre-2021 data / training data to check accuracy\n\n::: {#29b19705 .cell execution_count=7}\n``` {.python .cell-code}\n#set default as false\npre_2021[\"flag_prediction\"] = \"f\"\n#predicting flag status\nold_post = tweet_flagger(pre_2021)\n\n#confusion matrix\nfrequency_table = pd.crosstab(pre_2021['isFlagged'], old_post['flag_prediction'])\nprint(frequency_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nflag_prediction      f     t\nisFlagged                   \nf                53677  2590\nt                  120   184\n```\n:::\n:::\n\n\n- The model has high accuracy (95.48%), but this might be misleading because the dataset appears imbalanced (many more f cases than t cases).\n- The precision (6.63%) is low, meaning a large proportion of the flagged predictions are incorrect.\n- The recall (60.53%) is moderate, meaning the model identifies 60.53% of the actual flagged cases. \n-The F1-Score (11.91%) is low, indicating a poor balance between precision and recall.\n\nThese are not the greatest statistics but are comparable to the xgboost, logistic, and randomforest models I made previously for the same dataset. Likely due to the 304 vs ~24000 flagged to nonflagged inbalance in the dataset. I did indeed attempt SMOTE, oversampling, and undersampling to no avail.\n\n## Running it on the new post-2021 data!\n\n::: {#e40e42b6 .cell execution_count=8}\n``` {.python .cell-code}\n#time to test on post 2021\npost_2021 = pd.read_csv('media1/trump_post_2021.csv')\n#set default as false\npost_2021[\"flag_prediction\"] = \"f\"\n\n#predicting\nnew_post = tweet_flagger(post_2021)\n\n#confusion matrix\nfrequency_table = post_2021['flag_prediction'].value_counts()\nprint(frequency_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nflag_prediction\nf    17305\nt     2206\nName: count, dtype: int64\n```\n:::\n:::\n\n\nThe function classified 2206 of the post-2021 tweets as flagged and 17305 of them as nonflagged!\n\nLooking at some of the tweets the function predicted as flagged we see:\n\n**A giant Fake News Scam by CBS & 60 Minutes. Her REAL ANSWER WAS CRAZY, OR DUMB, so they actually REPLACED it with another answer in order to save her or, at least, make her look better. A FAKE NEWS SCAM, which is totally illegal. TAKE AWAY THE CBS LICENSE. Election Interference. She is a Moron, and the Fake News Media wants to hide that fact. An UNPRECEDENTED SCANDAL!!! The Dems got them to do this and should be forced to concede the Election? WOW!**\n\nOverall, very fun interesting project! Though I did do sentiment analysis on the side, I failed to implement it so that will be something to try for next time. \n\nAlso once again note that I attempted logistic, xgboost, and randomforest models that had similar results to this one in terms of predictive capability. If I had more time I would work more with the data to make it more balanced before modeling.\n\n",
    "supporting": [
      "01-project_files"
    ],
    "filters": [],
    "includes": {}
  }
}