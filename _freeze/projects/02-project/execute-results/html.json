{
  "hash": "ea3aa92019499fa60076b3f39d9193cc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Geospatial Predictions on Best Snow Crab Harvesting Grounds\ndate: \"03-18-2025\"\n---\n\n\n\nI used geo-statistical packages **geoR** and **gstat** to predict where Alaskan Snow Crab would be most dense based of the data spatial data gathered from the year 2010. \n\nI used the **Snow Crab Geospatial Data (1975-2018)** dataset posted by Matt Op on [kaggle](https://www.kaggle.com/datasets/mattop/snowcrab). This has already been data cleaned.\n\n# Importing Necessary Libraries and Dataset\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#libraries\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(rgl)\nlibrary(geoR)\nlibrary(gstat)\nlibrary(sp)\nlibrary(stats)\nlibrary(scatterplot3d)\nlibrary(lattice)\nlibrary(gridExtra)\n\n#dataset\ndata <- read_csv(\"media2/mfsnowcrab.csv\")\n```\n:::\n\n\n\n# Description of Dataset:\nThe description of the dataset variables is as follows: \n- Latitude: The latitude (decimal degrees) at the start of the haul. \n- Longitude: The longitude (decimal degrees) at the start of the haul. \n- Year: Year specimen was collected. \n- Name: The common name of the marine organism associated with the scientific name. \n- Sex: Gender of crab. \n- Bottom Depth: Meters (m). \n- Weighted average depth (m) and is calculated by adding gear depth to net height. \n- Surface Temperature: Surface temperature, in tenths of a degree, Celsius. \n- Bottom Temperature: Average temperature (in tenths of a degree Celsius) measured at the maximum depth of the trawl. \n- Haul: This number uniquely identifies a haul within a cruise. It is a sequential number, in chronological order of occurrence. \n- CPUE: Catch number per area the net swept in number/square nautical mile.\n\nWe will use CPUE as a response variable / attribute.\n\n# Basic EDA\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#dimensions\ndim(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 17927    11\n```\n\n\n:::\n\n```{.r .cell-code}\n#summary\nsummary(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id            latitude       longitude           year     \n Min.   :145878   Min.   :54.67   Min.   :-178.2   Min.   :1975  \n 1st Qu.:158286   1st Qu.:57.17   1st Qu.:-173.6   1st Qu.:1988  \n Median :163807   Median :58.34   Median :-170.9   Median :1998  \n Mean   :162675   Mean   :58.53   Mean   :-170.7   Mean   :1998  \n 3rd Qu.:168611   3rd Qu.:59.99   3rd Qu.:-167.9   3rd Qu.:2008  \n Max.   :173422   Max.   :62.04   Max.   :-158.4   Max.   :2018  \n     name               sex             bottom_depth    surface_temperature\n Length:17927       Length:17927       Min.   : 22.00   Min.   :-1.100     \n Class :character   Class :character   1st Qu.: 68.00   1st Qu.: 5.900     \n Mode  :character   Mode  :character   Median : 84.00   Median : 7.300     \n                                       Mean   : 90.01   Mean   : 7.124     \n                                       3rd Qu.:110.00   3rd Qu.: 8.600     \n                                       Max.   :276.00   Max.   :14.100     \n bottom_temperature      haul            cpue        \n Min.   :-2.100     Min.   :  1.0   Min.   :     52  \n 1st Qu.: 0.600     1st Qu.: 86.0   1st Qu.:    483  \n Median : 2.000     Median :127.0   Median :   3215  \n Mean   : 1.802     Mean   :123.8   Mean   :  32876  \n 3rd Qu.: 3.200     3rd Qu.:164.0   3rd Qu.:  21008  \n Max.   :10.000     Max.   :334.0   Max.   :5117962  \n```\n\n\n:::\n:::\n\n\nThere are 17927 observations with 11 columns. The variables all look to be in the same class.\n\n## Filtering the dataset to just 2010\nWe want to use only the 2010 data since it has the most observations. We also need to make sure the observation do not have duplicate haul numbers as the geospatial prediction method of kriging we use later does not allow for data that stems from the same location / observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#checking observation counts per year with no duplicate hauls\ndata %>%\n  distinct(haul, .keep_all = TRUE) %>%  \n  group_by(year) %>%                   \n  summarise(count = n()) %>%            \n  arrange(desc(count))            \n\n#filtering the data to only 2010 with unique data points only\ndata_2010 <- data %>%\n  filter(year == 2010) %>%  # Keep only 2010 data\n  group_by(haul) %>%  \n  mutate(count = n()) %>%    # Count occurrences of each haul\n  filter(count == 1 | row_number() == 1) %>%  # Keep unique hauls + first instance of duplicates\n  ungroup() %>%\n  select(-count)  # Remove count column\n\n#check for duplicates\nduplicates <- data_2010$haul[duplicated(data_2010$haul)]\nunique(duplicates)  # Shows unique haul numbers that are duplicated\n\n#check for nas\ncolSums(is.na(data_2010))\n```\n:::\n\n\nThere are no NA's. There are also no duplicates.\n\n## Looking at data_2010:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#dimensions\ndim(data_2010)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 210  11\n```\n\n\n:::\n\n```{.r .cell-code}\n#summary\nsummary(data_2010)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       id            latitude       longitude           year     \n Min.   :159664   Min.   :54.99   Min.   :-178.2   Min.   :2010  \n 1st Qu.:161313   1st Qu.:57.33   1st Qu.:-173.8   1st Qu.:2010  \n Median :162314   Median :58.68   Median :-171.2   Median :2010  \n Mean   :162649   Mean   :58.66   Mean   :-170.8   Mean   :2010  \n 3rd Qu.:163791   3rd Qu.:60.01   3rd Qu.:-168.0   3rd Qu.:2010  \n Max.   :166371   Max.   :62.00   Max.   :-161.6   Max.   :2010  \n     name               sex             bottom_depth    surface_temperature\n Length:210         Length:210         Min.   : 40.00   Min.   :1.00       \n Class :character   Class :character   1st Qu.: 68.00   1st Qu.:5.00       \n Mode  :character   Mode  :character   Median : 83.00   Median :6.55       \n                                       Mean   : 89.62   Mean   :6.32       \n                                       3rd Qu.:108.75   3rd Qu.:8.40       \n                                       Max.   :192.00   Max.   :9.70       \n bottom_temperature      haul             cpue          \n Min.   :-1.6000    Min.   : 16.00   Min.   :     52.0  \n 1st Qu.:-1.0000    1st Qu.: 86.25   1st Qu.:    677.2  \n Median : 0.6000    Median :139.50   Median :   5316.5  \n Mean   : 0.7481    Mean   :137.79   Mean   :  51539.8  \n 3rd Qu.: 2.1000    3rd Qu.:192.50   3rd Qu.:  28748.8  \n Max.   : 5.1000    Max.   :245.00   Max.   :1070004.0  \n```\n\n\n:::\n:::\n\n\n210 observations. Nice!\n\n# Histograms, ECDF, Scatterplot, Correlation Matrix\n\n## Correlation Matrix\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#correlation matrix\ncor_matrix <- cor(scale(data_2010[sapply(data_2010, is.numeric)]))\nhigh_corr_indices <- which(abs(cor_matrix) > 0.7, arr.ind = TRUE)\n# Extract the names of the variables involved in the high correlations\nhigh_corr_pairs <- data.frame(\n  var1 = rownames(cor_matrix)[high_corr_indices[, 1]],\n  var2 = colnames(cor_matrix)[high_corr_indices[, 2]],\n  correlation = cor_matrix[high_corr_indices]\n)\n\n#response correlations\ncor_cpue <- cor_matrix[\"cpue\", ]\n#which are the largest one\nsort(abs(cor_cpue), decreasing = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               cpue            latitude                haul surface_temperature \n         1.00000000          0.48166915          0.42204061          0.36076845 \n          longitude                  id  bottom_temperature        bottom_depth \n         0.32925130          0.27546297          0.21331928          0.02460697 \n```\n\n\n:::\n:::\n\n\nI want to use bottom_temperature, bottom_depth, surface_temperature as predictors for the method of co-kriging later as this method requires strongly correlated variables.\n\n## Histograms\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2,2))\n#histograms\nhist(data_2010$cpue, \n     main = \"Histogram of Catch Number per Area\", \n     xlab = \"CPUE\")\n\nhist(data_2010$bottom_temperature, \n     main = \"Histogram of Bottom Temperature\", \n     xlab = \"bottom_temperature\")\n\nhist(data_2010$bottom_depth, \n     main = \"Histogram of Bottom Depth (m)\", \n     xlab = \"Bottom Depth (m)\")\n\nhist(data_2010$surface_temperature, \n     main = \"Histogram of Surface Temperature\", \n     xlab = \"Surface Temperature\")\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nLooks pretty normal except for the CPUE, this one is skewed right. Let's test normality for all though.\n\n## Tests for Normality\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshapiro.test(data_2010$cpue)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  data_2010$cpue\nW = 0.43008, p-value < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(data_2010$bottom_temperature)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  data_2010$bottom_temperature\nW = 0.92708, p-value = 1.051e-08\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(data_2010$bottom_depth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  data_2010$bottom_depth\nW = 0.95349, p-value = 2.487e-06\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro.test(data_2010$surface_temperature)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  data_2010$surface_temperature\nW = 0.94799, p-value = 7.044e-07\n```\n\n\n:::\n:::\n\n\nSince the p-value is less than 0.05 for all of them, none of them are normal. I should probably log transform the variables later when doing analysis.\n\n## h-scatterplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#hscatterplot\n# Produces h-scatterplots, where point pairs having specific separation distances are plotted\n# first argument is the formula / variable used\n# second is the data set\n# third is the breaks in the data we are detecting in this case 0 to 1 by 0.1\ncoordinates(data_2010) <- ~longitude+latitude\ndist_mat <- dist(coordinates(data_2010))\n#max distance\ndist_max <- max(dist_mat)\nqq <- hscat(cpue~1, data_2010, seq(0, 20, by = 2))\nplot(qq, main=\"h-scatterplots\")\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nThis is a lagged scatterplot matrix that shows the relationship between \"cpue\" (catch per unit effort) at different distance lags. Each panel represents a scatterplot of cpue values at different lag intervals, and the correlation coefficient is shown for each panel. Since the first panel has the strongest correlation of 0.238 we can conclude that cpue values at nearby locations tend to be somewhat similar and as distance increases this correlation decreases.\n\n## Bubble Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#bubble plot\nresponse <- as.data.frame(cbind(coordinates(data_2010), data_2010$cpue))\nbubble <- as.geodata(response)\npoints(bubble)\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThis bubble plot shows the locations of all the observations and their outcome variable magnitudes. The bigger the circle the bigger the cpue variable.\n\n# Generating an Emperical Semivariogram for Kriging.\nHere we generate an emperical semivariogram. A semivariogram is a tool used in geostatistics to measure how similar (or different) data points are based on the distance between them. It helps us understand spatial patterns in data, such as whether nearby locations have similar values or if there is a predictable structure across space. The x-axis is the distance lag between points. The y-axis is semivariance which is how different the points tend to be. This is calculated by taking the difference in values and squaring them.\n\nWe need this because prediction using kriging relies on the idea that nearby locations have similar values, and the semivariogram quantifies this relationship by showing how variance (difference) increases with distance. Later using this model we make, kriging will assign weights to data points based on their distances and this semivariogram model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#log transform since skewed\n#locations already set\nc_stat <- gstat(id=\"log(cpue)\", formula = log(cpue)~1, \n                data = data_2010)\n#calc / plot variogram\n#classical\nvar1 <- variogram(c_stat)\np1 <- plot(var1, main = \"Emperical Semi-Variogram Classic\")\n\n#robust\nvar2 <- variogram(c_stat, cressie = TRUE)\np2 <- plot(var2, main = \"Emperical Semi-Variogram Robust\")\n\n# 4 dir\nvar_dir <- variogram(c_stat, alpha=c(0,45,90,135))\np3 <- plot(var_dir)\n\n#3d scatterplot\np4 <- scatterplot3d(coordinates(data_2010)[,1], coordinates(data_2010)[,2], \n              log(data_2010$cpue))\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#another look\n# plot3d(coordinates(data_2010)[,1], coordinates(data_2010)[,2], \n#               log(data_2010$cpue))\n#looks fine\n\n#Crossvalidation for 3 models Classical\n#Fitting Gaussian Model \ng.fit <- fit.variogram(var1, vgm(1,\"Gau\",3,7))\np5 <- plot(var1, g.fit, main = \"Gaussian\")\ncv_gau <- krige.cv(log(cpue)~1, data_2010, \n                   model = g.fit, nfold = nrow(data_2010))\n\n#Fitting Exponential Model\ne.fit <- fit.variogram(var1, vgm(1,\"Exp\",3,7))\np6 <- plot(var1, e.fit, main = \"Exponential\")\ncv_exp <- krige.cv(log(cpue)~1, data_2010, \n                   model = e.fit, nfold = nrow(data_2010))\n\n#Fitting Spherical Model\ns.fit <- fit.variogram(var1, vgm(1,\"Sph\",3,7))\np7 <- plot(var1, s.fit, main = \"Spherical\")\ncv_sph <- krige.cv(log(cpue)~1, data_2010, \n                   model = s.fit, nfold = nrow(data_2010))\n\n#press comparison\npress_gau <- sum(cv_gau$residual^2) / nrow(data_2010)\npress_exp <- sum(cv_exp$residual^2) / nrow(data_2010)\npress_sph <- sum(cv_sph$residual^2) / nrow(data_2010)\nc(press_gau, press_exp, press_sph)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.449019 1.471420 1.448176\n```\n\n\n:::\n\n```{.r .cell-code}\n#Crossvalidation for 3 models Robust\n#Fitting Gaussian Model \ng.fit2 <- fit.variogram(var2, vgm(1,\"Gau\",3,7))\np8 <- plot(var2, g.fit2, main = \"Gaussian\")\ncv_gau2 <- krige.cv(log(cpue)~1, data_2010, \n                   model = g.fit2, nfold = nrow(data_2010))\n\n#Fitting Exponential Model\ne.fit2 <- fit.variogram(var2, vgm(1,\"Exp\",3,7))\np9 <- plot(var2, e.fit2, main = \"Exponential\")\ncv_exp2 <- krige.cv(log(cpue)~1, data_2010, \n                   model = e.fit2, nfold = nrow(data_2010))\n\n#Fitting Spherical Model\ns.fit2 <- fit.variogram(var2, vgm(1,\"Sph\",3,7))\np10 <- plot(var2, s.fit2, main = \"Spherical\")\ncv_sph2 <- krige.cv(log(cpue)~1, data_2010, \n                   model = s.fit2, nfold = nrow(data_2010))\n\n#press comparison\npress_gau2 <- sum(cv_gau2$residual^2) / nrow(data_2010)\npress_exp2 <- sum(cv_exp2$residual^2) / nrow(data_2010)\npress_sph2 <- sum(cv_sph2$residual^2) / nrow(data_2010)\nc(press_gau2, press_exp2, press_sph2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.462165 1.446201 1.417918\n```\n\n\n:::\n\n```{.r .cell-code}\n#only plot the best one\np10\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-11-2.png){width=672}\n:::\n:::\n\n\n\nFirst, I plot the sample variogram given the dataset. We first make a gstat object that log transforms the cpue variable. We then plot the variogram, which can be further adjusted using classical or robust estimators to improve the next step of model fit. We then need to fit a variogram model to this plot so that we can make predictions in kriging later. I use two types of sample variograms and three type of variogram models here and cross-validate to find that the spherical variogram function fit on the sample variogram using robust estimators minimizes the prediction squared error. Notice that we have a 3D scatterplot of the data. This is used to identify spatial trends in the data to see if data detrending is required to meet the requirements of ordinary kriging which is a constant mean. Luckily there is no trend.\n\n# Inverse Distance Interpolation Predictions\nWe first try Inverse Distance Interpolation Prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#making grid\nx_val <- seq(min(coordinates(data_2010)[,1]), max(coordinates(data_2010)[,1]), \n             length.out = 60)\ny_val <- seq(min(coordinates(data_2010)[,2]), max(coordinates(data_2010)[,2]), \n             length.out = 60)\ngrid <- expand.grid(longitude = x_val, latitude = y_val)\ncoordinates(grid) <- ~longitude+latitude\n\n#plot grid and obs\n# plot(grid)\n# points(data_2010)\n\n#idw pred\nidw.out <- idw(log(cpue)~1, data_2010, grid)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#collapse prediction to matrix\nqqq <- matrix(idw.out$var1.pred, length(x_val), length(y_val))\n\n#removing NAs\n#edited it based off the name of coordinate columns\nmask <- function(predictions, pts, grid.x, grid.y, win.size) {\n  new.qqq <- predictions\n  m <- nrow(predictions)\n  n <- ncol(predictions)\n  for (i in 1:m) {\n    for (j in 1:n) {\n      x.min <- i-win.size\n      y.min <- j-win.size\n      x.max <- i+win.size\n      y.max <- j+win.size\n      if (x.min < 1) { x.min <- 1 }; if (y.min < 1) { y.min <- 1 }\n      if (x.max > length(grid.x)) { x.max <- length(grid.x) };\n      if (y.max > length(grid.y)) { y.max <- length(grid.y) }\n      candidates <- pts[which(pts$longitude >= grid.x[x.min] \n                              & pts$longitude <= grid.x[x.max]),]\n      thePoints <- candidates[which(candidates$latitude >= grid.y[y.min] \n                                    & candidates$latitude <= grid.y[y.max]),]\n      if (nrow(thePoints) == 0) {\n        new.qqq[i,j] <- NA\n      }\n    }\n  }\n  return(new.qqq)\n}\nd <- mask(qqq, data_2010, x_val, y_val, 5)\n\n#rastermap\nimage(x_val, y_val, d, xlab=\"West to East\", ylab=\"South to North\")\n#Bubble plot:\npoints(data_2010, cex=log(data_2010$cpue) / mean(log(data_2010$cpue)), pch=19)\n#Add contours:\ncontour(x_val, y_val, qqq, add=TRUE, col=\"black\", labcex=1)\ntitle(main=\"Log_cpue IDW Predictions\", sub = \"Contour lines\")\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nInverse Distance Interpolation predicting is a method used to estimate values at unknown locations based on nearby known values. It assumes that closer points have more influence on the predicted value than farther points.\n\nThe mask function is implemented to remove all areas on the raster map outside of the sample domain as all spatial prediction is interpolation, as in you can only make predictions within the given domain of points. \n\n# Predictions using Kriging:\nWe now use kriging to make predictions. Kriging is an advanced geostatistical interpolation method used to predict unknown values at specific locations based on known data points. Unlike Inverse Distance Weighting (IDW), kriging considers both distance and spatial correlation (trends and patterns in data), making it more accurate for many applications.\n\nWe use different types of kriging below:\n- Ordinary Kriging: assumes a constant but unknown mean\n- Universal Kriging: assumes a non-constant and unknown mean, or trended data\n- Co-Kriging: uses correlated variables to help contribute to main variable predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#we will just use ordinary, universal, and cokriging\n#ordinary kriging\nok <- krige(id=\"log_cpue\", log(cpue)~1, model=s.fit2, data_2010, \n            newdata=grid) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[using ordinary kriging]\n```\n\n\n:::\n\n```{.r .cell-code}\n#universal is just assuming trend (make sure grid and gstat obj col same)\nuk <- krige(id=\"log_cpue\", log(cpue)~longitude+latitude, model=s.fit2, \n                  data_2010, newdata=grid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[using universal kriging]\n```\n\n\n:::\n\n```{.r .cell-code}\n#cokriging\n#setup\n#co-kriging use bottom temp and surface temp since they are the most correlated\ng1 <- gstat(id=\"log_cpue\", formula = log(cpue)~1, data = data_2010)\n# Append transformed bottom_temp, depth, and surface_temp\n# shifting the negative values in bottom temp\nmin_value <- min(data_2010$bottom_temperature, na.rm = TRUE)\ng1 <- gstat(g1, id=\"logshift_bottom_temperature\",\n            formula = log(bottom_temperature - min_value + 1)~1,\n            data = data_2010)\ng1 <- gstat(g1, id=\"log_bottom_depth\", formula = log(bottom_depth)~1,\n            data = data_2010)\n# g1 <- gstat(g1, id=\"log_surface_temperature\",\n#             formula = log(surface_temperature)~1, data = data_2010)\n\n#variogram co krig, (can you detrend cokriging predictors? ask)\nvar_co <- variogram(g1)\np11 <- plot(var_co, main = \"Emperical Semivariogram Co-Krig\")\nvm_fit <- fit.lmc(var_co, g1, model=s.fit2)\nplot(var_co, vm_fit)\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nI ended up having to use just the log transformed bottom temperature and bottom depth variables for co-kriging as the cross validation function was unable to converge when using all three. We cross validate co-kriging, ordinary kriging, and universal kriging below.\n\n## Cross Validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ordinary kriging\ncv_ok <- krige.cv(log(cpue)~1, data_2010, model=s.fit2, \n                  nfold=nrow(data_2010)) \npress_ok <- sum(cv_ok$residual^2) / nrow(data_2010)\n\n#universal cv\ncv_uk <- krige.cv(log(cpue)~longitude+latitude, data_2010, model=s.fit2, \n                  nfold=nrow(data_2010)) \npress_uk <- sum(cv_ok$residual^2) / nrow(data_2010)\n\n#cokriging cv\ncv_ck <- gstat.cv(vm_fit, verbose = FALSE)\npress_ck <- sum(cv_ck$residual^2) / nrow(data_2010)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nc(press_ok, press_uk, press_ck)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.417918 1.417918 1.282675\n```\n\n\n:::\n:::\n\n\nCo-kriging is the best at minimizing prediction squared error out of the three we tested so we will make predictions using it.\n\n## Predictions for Co-Kriging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Cokriging predictions:\nck <- predict(vm_fit, grid, verbose = FALSE)\n\n#collapse\npred_mat <- matrix(ck$log_cpue.pred, length(x_val), length(y_val))\n```\n:::\n\n\nWe leave the predictions in matrix form then plot it.\n\n# Raster Map from Co-Kriging\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#use interpolating insurer from earlier\nd2 <- mask(pred_mat, data_2010, x_val, y_val, 5)\n\n#Raster\nimage(x_val, y_val, d2, xlab=\"West to East\", ylab=\"South to North\")\n#Bubble plot:\npoints(data_2010, cex=log(data_2010$cpue) / mean(log(data_2010$cpue)), pch=19)\n#Add contours:\ncontour(x_val, y_val, pred_mat, add=TRUE, col=\"black\", labcex=1)\ntitle(main=\"Log_cpue Co-Krig Predictions\", sub = \"Contour lines\")\n```\n\n::: {.cell-output-display}\n![](02-project_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nSimilarly, we mask points that are outside the domain of the dataset. This raster map shows the locations where Alaskan Snow Crab concentrations would theoretically be highest!",
    "supporting": [
      "02-project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}