[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Richard Xu",
    "section": "",
    "text": "Richard Xu is a senior at the University of California, Los Angeles, pursuing a double major in Statistics & Data Science and Economics. Outside of academics and organizing club activities, he loves exploring the underwater world through scuba diving and capturing intricate details of nature through macro photography."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Richard Xu",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles | Los Angeles, CA\n\n\n\nDegree\nDates\n\n\n\n\nB.S. in Statistics & Data Science\nSept 2021 - June 2025\n\n\nB.A. in Economics\nSept 2021 - June 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Richard Xu",
    "section": "Experience",
    "text": "Experience\nNikira Labs | Sales & Marketing Intern | July 2023 - Present"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Richard Xu",
    "section": "",
    "text": "Richard Xu is a senior at the University of California, Los Angeles, pursuing a double major in Statistics & Data Science and Economics. Outside of academics and organizing club activities, he loves exploring the underwater world through scuba diving and capturing intricate details of nature through macro photography."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nTF-IDF on Trump Tweets\n\n\nDec 7, 2024\n\n\n\n\nGeospatial Predictions on Best Snow Crab Harvesting Grounds\n\n\nMar 18, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#university-of-california-los-angeles-los-angeles-ca",
    "href": "index.html#university-of-california-los-angeles-los-angeles-ca",
    "title": "Richard Xu",
    "section": "University of California, Los Angeles | Los Angeles, CA",
    "text": "University of California, Los Angeles | Los Angeles, CA"
  },
  {
    "objectID": "index.html#b.s.-in-statistics-data-science-sept-2021---june-2025",
    "href": "index.html#b.s.-in-statistics-data-science-sept-2021---june-2025",
    "title": "Richard Xu",
    "section": "B.S. in Statistics & Data Science | Sept 2021 - June 2025",
    "text": "B.S. in Statistics & Data Science | Sept 2021 - June 2025\nB.A. in Economics | Sept 2021 - June 2025"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "",
    "section": "",
    "text": "Here is my project 1"
  },
  {
    "objectID": "projects/01-project.html",
    "href": "projects/01-project.html",
    "title": "TF-IDF on Trump Tweets",
    "section": "",
    "text": "I made a model to predictively flag Trump’s post-2021 tweets based off the flagged or non-flagged statuses of his pre-2021 tweets. While the model did not have the best results, I learned a lot about how to do text-based analysis!\nTo understand the topic, here’s a video I made explaining how TF-IDF works!"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "",
    "section": "",
    "text": "#look at notes from jose’s repo\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/01-project.html#running-tf-idf-on-the-flagged-data",
    "href": "projects/01-project.html#running-tf-idf-on-the-flagged-data",
    "title": "TF-IDF on Trump Tweets",
    "section": "Running TF-IDF on the Flagged Data",
    "text": "Running TF-IDF on the Flagged Data\n\n#tfidf isFlagged\nfrom sklearn.feature_extraction.text import TfidfVectorizer #scikitlearn library, feature_extraction module, TfidfVectorizer Class \n\ncustom_stop_words = [\"t.co\", \"https\", \"http\", \"rt\", \"amp\", \"www\", \"co\", \"com\"]\ndefault_stop_words = TfidfVectorizer(stop_words='english').get_stop_words()\ncombined_stop_words = list(default_stop_words.union(custom_stop_words))\n\n# Combine custom stop words with the default English stop words\ntfidf = TfidfVectorizer(\n    stop_words=combined_stop_words, \n    max_features=3000,\n    ngram_range = (1,3)\n)\n\n#Fit the TfidfVectorizer to the data\nresult = tfidf.fit_transform(flagged_pre_2021.text)\nkeys = list(tfidf.vocabulary_.keys())\n#print(keys)\n\n# # get indexing for first 30 values\nprint('\\nWord indexes:')\nfirst_few = list(tfidf.vocabulary_.items())[:30]\nfor key, value in first_few:\n    print(f\"{key}: {value}\")\n#print(tfidf.vocabulary_)\n \n# # display tf-idf values\nprint('\\ntf-idf value:')\nprint(result)\n\n#check tfidf matrix dimensions\n#number of documents by number of features\nprint(result.toarray().shape)\n\n\nWord indexes:\nnevada: 2598\ncesspool: 445\nfake: 1622\nvotes: 2912\nfinding: 1698\nthings: 2828\ncesspool fake: 446\nfake votes: 1648\nfinding things: 1703\ncesspool fake votes: 447\nfake votes mschlapp: 1649\nfinding things released: 1704\npennsylvania: 2629\nwatching: 2944\nballot: 77\ncount: 739\nillegal: 2395\ncountry: 799\nballot count: 78\ncount unthinkable: 751\nillegal country: 2398\ncount unthinkable illegal: 752\nteamtrump: 2817\nlive: 2535\npress: 2677\nconference: 656\nchairwoman: 448\nteamtrump live: 2818\nconference kayleigh: 659\nchairwoman ronna: 451\n\ntf-idf value:\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 5681 stored elements and shape (304, 3000)&gt;\n  Coords    Values\n  (0, 2598) 0.24973806396072906\n  (0, 445)  0.3243383980144909\n  (0, 1622) 0.1953010208212467\n  (0, 2912) 0.15560878149712018\n  (0, 1698) 0.28703823098760994\n  (0, 2828) 0.23773009225078487\n  (0, 446)  0.3243383980144909\n  (0, 1648) 0.3243383980144909\n  (0, 1703) 0.3243383980144909\n  (0, 447)  0.3243383980144909\n  (0, 1649) 0.3243383980144909\n  (0, 1704) 0.3243383980144909\n  (1, 2629) 0.21824894321863983\n  (1, 2944) 0.3042825942418986\n  (1, 77)   0.23426573698111625\n  (1, 739)  0.28826580047942213\n  (1, 2395) 0.29577235950880687\n  (1, 799)  0.2515966410114136\n  (1, 78)   0.3582826577402044\n  (1, 751)  0.38412379650359346\n  (1, 2398) 0.38412379650359346\n  (1, 752)  0.38412379650359346\n  (2, 2817) 0.28395367876274485\n  (2, 2535) 0.28395367876274485\n  (2, 2677) 0.3123345034376439\n  : :\n  (303, 2496)   0.1327285528588964\n  (303, 2856)   0.12005888852084247\n  (303, 2102)   0.14298870514984965\n  (303, 2539)   0.14011030504238925\n  (303, 2559)   0.12490863886921652\n  (303, 2703)   0.17645320454777802\n  (303, 2977)   0.16304012167139642\n  (303, 2562)   0.146141635735278\n  (303, 2733)   0.16907145236428517\n  (303, 2840)   0.16907145236428517\n  (303, 2866)   0.16304012167139642\n  (303, 1925)   0.1859699383004036\n  (303, 2498)   0.1859699383004036\n  (303, 489)    0.3987660423535704\n  (303, 966)    0.1993830211767852\n  (303, 2109)   0.1993830211767852\n  (303, 490)    0.1993830211767852\n  (303, 967)    0.1993830211767852\n  (303, 1846)   0.1993830211767852\n  (303, 1928)   0.1993830211767852\n  (303, 2110)   0.1993830211767852\n  (303, 491)    0.1993830211767852\n  (303, 968)    0.1993830211767852\n  (303, 1847)   0.1993830211767852\n  (303, 1929)   0.1993830211767852\n(304, 3000)\n\n\nWhen looking at the coordinate, the left coordinate is the document, or in our case tweet, that the word/phrase is in. The right coordinate is a word/phrase which corresponds to the tfidf vocabulary_ dictionary. Finally the values column gives us the TF-IDF scores/values: the higher it is, the more important it is!"
  },
  {
    "objectID": "projects/01-project.html#running-tf-idf-on-the-non-flagged-data",
    "href": "projects/01-project.html#running-tf-idf-on-the-non-flagged-data",
    "title": "TF-IDF on Trump Tweets",
    "section": "Running TF-IDF on the Non-Flagged Data",
    "text": "Running TF-IDF on the Non-Flagged Data\n\n#tfidf is not Flagged\n#we add these custom ones to remove links that Trump posts\ncustom_stop_words = [\"t.co\", \"https\", \"http\", \"rt\", \"amp\", \"www\", \"co\", \"com\"]\ndefault_stop_words = TfidfVectorizer(stop_words='english').get_stop_words()\ncombined_stop_words = list(default_stop_words.union(custom_stop_words))\n\n# Combine custom stop words with the default English stop words\ntfidf2 = TfidfVectorizer(\n    stop_words=combined_stop_words,  \n    max_features=3000, #3000 features to look for\n    ngram_range = (1,3) #each feature can be 1 to 3 words long (to capture nuance)\n)\n\n# Fit the TfidfVectorizer to the data\nresult2 = tfidf2.fit_transform(n_flagged_pre_2021.text)\n\n#the key features\nkeys2 = list(tfidf2.vocabulary_.keys())\n#print(keys2)\n\n# # get indexing\n# print('\\nWord indexes:')\n# print(tfidf.vocabulary_)\n \n# # display tf-idf values\n# print('\\ntf-idf value:')\n# print(result)"
  },
  {
    "objectID": "projects/01-project.html#taking-the-difference-between-the-two-word-sets",
    "href": "projects/01-project.html#taking-the-difference-between-the-two-word-sets",
    "title": "TF-IDF on Trump Tweets",
    "section": "Taking the difference between the two word sets",
    "text": "Taking the difference between the two word sets\n\n#difference between vocab sets\nkeys_set = set(keys)\nkeys2_set = set(keys2)\ndifference_keys = keys_set.difference(keys2_set)\n#print(difference_keys)"
  },
  {
    "objectID": "projects/01-project.html#quick-visualization-of-word-overlap",
    "href": "projects/01-project.html#quick-visualization-of-word-overlap",
    "title": "TF-IDF on Trump Tweets",
    "section": "Quick visualization of word overlap",
    "text": "Quick visualization of word overlap\n\nfrom matplotlib_venn import venn2\nimport matplotlib.pyplot as plt\n\n# Create sets from your word lists\nflagged_set = keys_set  # Words in the flagged tweets\nnon_flagged_set = keys2_set  # Words in non-flagged tweets\n\n# Create a Venn diagram\nvenn = venn2([flagged_set, non_flagged_set], set_labels=('Flagged Set', 'Non-Flagged Set'))\nvenn.get_patch_by_id('10').set_color('lightgreen')  # Color for flagged set\nvenn.get_patch_by_id('01').set_color('lightgray')  # Color for non-flagged set\nvenn.get_patch_by_id('11').set_color('red')  # Color for intersection\n\nplt.show()\n\n\n\n\n\n\n\n\nThe values we will use to predict if a tweet is flagged are the ones in green.\nI take the dictionary “difference” that I just found which is the words in the isFlagged tweets but not the non-flagged ones and then place that set into a function “tweet_flagger”. This essentially searches through all the post-2021 tweets and changes their flag_prediction value to True if it detects any of the words in the dictionary.\n\nimport string\n\ndef tweet_flagger(df):\n    flagged_words = difference_keys\n    for index, row in df.iterrows():\n        tweet = row['text']  # Replace 'tweet' with the actual column name that holds the tweet text\n        tweet = tweet.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n        separated_words = tweet.split()  # Split the tweet into words\n\n        # Check if any word in the tweet is in the flagged dictionary\n        if any(word.lower() in flagged_words for word in separated_words):\n            df.at[index, 'flag_prediction'] = 't'  # Update flag_prediction to 't'\n    return df[['text', 'flag_prediction']]"
  },
  {
    "objectID": "projects/01-project.html#running-the-function-on-the-pre-2021-data-training-data-to-check-accuracy",
    "href": "projects/01-project.html#running-the-function-on-the-pre-2021-data-training-data-to-check-accuracy",
    "title": "TF-IDF on Trump Tweets",
    "section": "Running the function on the pre-2021 data / training data to check accuracy",
    "text": "Running the function on the pre-2021 data / training data to check accuracy\n\n#set default as false\npre_2021[\"flag_prediction\"] = \"f\"\n#predicting flag status\nold_post = tweet_flagger(pre_2021)\n\n#confusion matrix\nfrequency_table = pd.crosstab(pre_2021['isFlagged'], old_post['flag_prediction'])\nprint(frequency_table)\n\nflag_prediction      f     t\nisFlagged                   \nf                53677  2590\nt                  120   184\n\n\n\nThe model has high accuracy (95.48%), but this might be misleading because the dataset appears imbalanced (many more f cases than t cases).\nThe precision (6.63%) is low, meaning a large proportion of the flagged predictions are incorrect.\nThe recall (60.53%) is moderate, meaning the model identifies 60.53% of the actual flagged cases. -The F1-Score (11.91%) is low, indicating a poor balance between precision and recall.\n\nThese are not the greatest statistics but are comparable to the xgboost, logistic, and randomforest models I made previously for the same dataset. Likely due to the 304 vs ~24000 flagged to nonflagged inbalance in the dataset. I did indeed attempt SMOTE, oversampling, and undersampling to no avail."
  },
  {
    "objectID": "projects/01-project.html#running-it-on-the-new-post-2021-data",
    "href": "projects/01-project.html#running-it-on-the-new-post-2021-data",
    "title": "TF-IDF on Trump Tweets",
    "section": "Running it on the new post-2021 data!",
    "text": "Running it on the new post-2021 data!\n\n#time to test on post 2021\npost_2021 = pd.read_csv('media1/trump_post_2021.csv')\n#set default as false\npost_2021[\"flag_prediction\"] = \"f\"\n\n#predicting\nnew_post = tweet_flagger(post_2021)\n\n#confusion matrix\nfrequency_table = post_2021['flag_prediction'].value_counts()\nprint(frequency_table)\n\nflag_prediction\nf    17305\nt     2206\nName: count, dtype: int64\n\n\nThe function classified 2206 of the post-2021 tweets as flagged and 17305 of them as nonflagged!\nLooking at some of the tweets the function predicted as flagged we see:\nA giant Fake News Scam by CBS & 60 Minutes. Her REAL ANSWER WAS CRAZY, OR DUMB, so they actually REPLACED it with another answer in order to save her or, at least, make her look better. A FAKE NEWS SCAM, which is totally illegal. TAKE AWAY THE CBS LICENSE. Election Interference. She is a Moron, and the Fake News Media wants to hide that fact. An UNPRECEDENTED SCANDAL!!! The Dems got them to do this and should be forced to concede the Election? WOW!\nOverall, very fun interesting project! Though I did do sentiment analysis on the side, I failed to implement it so that will be something to try for next time.\nAlso once again note that I attempted logistic, xgboost, and randomforest models that had similar results to this one in terms of predictive capability. If I had more time I would work more with the data to make it more balanced before modeling."
  },
  {
    "objectID": "projects/01-project.html#importing-modules-reading-dataset-indexing-dataset",
    "href": "projects/01-project.html#importing-modules-reading-dataset-indexing-dataset",
    "title": "TF-IDF on Trump Tweets",
    "section": "Importing Modules, Reading Dataset, Indexing Dataset",
    "text": "Importing Modules, Reading Dataset, Indexing Dataset\n\nimport pandas as pd\nimport numpy as np\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\npre_2021 = pd.read_csv('media1/tweets_01-08-2021.csv')\n#print(pre_2021.text)\n\n#two dataframes\nindex = pre_2021['isFlagged'] == 't'\nflagged_pre_2021 = pre_2021[index]\nn_flagged_pre_2021 = pre_2021[~index]"
  },
  {
    "objectID": "index.html#clubs",
    "href": "index.html#clubs",
    "title": "Richard Xu",
    "section": "Clubs",
    "text": "Clubs\n\nBruin Earth Solutions Aquaponics\nProject Lead\nSeptember 2024 – Present\nUCLA Club Beach Volleyball\nMember\nSeptember 2021 – Present"
  },
  {
    "objectID": "index.html#job-experience",
    "href": "index.html#job-experience",
    "title": "Richard Xu",
    "section": "Job Experience",
    "text": "Job Experience\n\nNikira Labs\nSales & Marketing Analyst Intern\nJuly 2023 - Present"
  },
  {
    "objectID": "hobbies/tidepooling.html",
    "href": "hobbies/tidepooling.html",
    "title": "Tidepooling Finds Interactive Map",
    "section": "",
    "text": "library(leaflet)\n\nWarning: package 'leaflet' was built under R version 4.2.3\n\nlibrary(tigris)\n\nWarning: package 'tigris' was built under R version 4.2.3\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(sf)\n\nLinking to GEOS 3.9.3, GDAL 3.5.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\ncalifornia &lt;- states(cb = TRUE)\n\nRetrieving data for the year 2021\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\ncalifornia_sf &lt;- california[california$STUSPS == \"CA\", ]\n\n# Create a leaflet map and add the California state geometry\nleaflet(california_sf) %&gt;%\n  addTiles() %&gt;%  # Adds default OpenStreetMap tiles\n  addPolygons(fillColor = \"lightblue\",\n              color = \"black\",\n              weight = 2,\n              opacity = 1,\n              fillOpacity = 0.5,\n              popup = ~STUSPS)  # Displays the state abbreviation as a popup\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'"
  },
  {
    "objectID": "hobbies.html",
    "href": "hobbies.html",
    "title": "",
    "section": "",
    "text": "Tidepooling Finds Interactive Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/02-project.html",
    "href": "projects/02-project.html",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "",
    "text": "I used geo-statistical packages geoR and gstat to predict where Alaskan Snow Crab would be most dense based of the data spatial data gathered from the year 2010.\nI used the Snow Crab Geospatial Data (1975-2018) dataset posted by Matt Op on kaggle. This has already been data cleaned."
  },
  {
    "objectID": "projects/02-project.html#filtering-the-dataset-to-just-2010",
    "href": "projects/02-project.html#filtering-the-dataset-to-just-2010",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Filtering the dataset to just 2010",
    "text": "Filtering the dataset to just 2010\nWe want to use only the 2010 data since it has the most observations. We also need to make sure the observation do not have duplicate haul numbers as the geospatial prediction method of kriging we use later does not allow for data that stems from the same location / observation.\n\n#checking observation counts per year with no duplicate hauls\ndata %&gt;%\n  distinct(haul, .keep_all = TRUE) %&gt;%  \n  group_by(year) %&gt;%                   \n  summarise(count = n()) %&gt;%            \n  arrange(desc(count))            \n\n#filtering the data to only 2010 with unique data points only\ndata_2010 &lt;- data %&gt;%\n  filter(year == 2010) %&gt;%  # Keep only 2010 data\n  group_by(haul) %&gt;%  \n  mutate(count = n()) %&gt;%    # Count occurrences of each haul\n  filter(count == 1 | row_number() == 1) %&gt;%  # Keep unique hauls + first instance of duplicates\n  ungroup() %&gt;%\n  select(-count)  # Remove count column\n\n#check for duplicates\nduplicates &lt;- data_2010$haul[duplicated(data_2010$haul)]\nunique(duplicates)  # Shows unique haul numbers that are duplicated\n\n#check for nas\ncolSums(is.na(data_2010))\n\nThere are no NA’s. There are also no duplicates."
  },
  {
    "objectID": "projects/02-project.html#looking-at-data_2010",
    "href": "projects/02-project.html#looking-at-data_2010",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Looking at data_2010:",
    "text": "Looking at data_2010:\n\n#dimensions\ndim(data_2010)\n\n[1] 210  11\n\n#summary\nsummary(data_2010)\n\n       id            latitude       longitude           year     \n Min.   :159664   Min.   :54.99   Min.   :-178.2   Min.   :2010  \n 1st Qu.:161313   1st Qu.:57.33   1st Qu.:-173.8   1st Qu.:2010  \n Median :162314   Median :58.68   Median :-171.2   Median :2010  \n Mean   :162649   Mean   :58.66   Mean   :-170.8   Mean   :2010  \n 3rd Qu.:163791   3rd Qu.:60.01   3rd Qu.:-168.0   3rd Qu.:2010  \n Max.   :166371   Max.   :62.00   Max.   :-161.6   Max.   :2010  \n     name               sex             bottom_depth    surface_temperature\n Length:210         Length:210         Min.   : 40.00   Min.   :1.00       \n Class :character   Class :character   1st Qu.: 68.00   1st Qu.:5.00       \n Mode  :character   Mode  :character   Median : 83.00   Median :6.55       \n                                       Mean   : 89.62   Mean   :6.32       \n                                       3rd Qu.:108.75   3rd Qu.:8.40       \n                                       Max.   :192.00   Max.   :9.70       \n bottom_temperature      haul             cpue          \n Min.   :-1.6000    Min.   : 16.00   Min.   :     52.0  \n 1st Qu.:-1.0000    1st Qu.: 86.25   1st Qu.:    677.2  \n Median : 0.6000    Median :139.50   Median :   5316.5  \n Mean   : 0.7481    Mean   :137.79   Mean   :  51539.8  \n 3rd Qu.: 2.1000    3rd Qu.:192.50   3rd Qu.:  28748.8  \n Max.   : 5.1000    Max.   :245.00   Max.   :1070004.0  \n\n\n210 observations. Nice!"
  },
  {
    "objectID": "projects/02-project.html#correlation-matrix",
    "href": "projects/02-project.html#correlation-matrix",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\n\n#correlation matrix\ncor_matrix &lt;- cor(scale(data_2010[sapply(data_2010, is.numeric)]))\nhigh_corr_indices &lt;- which(abs(cor_matrix) &gt; 0.7, arr.ind = TRUE)\n# Extract the names of the variables involved in the high correlations\nhigh_corr_pairs &lt;- data.frame(\n  var1 = rownames(cor_matrix)[high_corr_indices[, 1]],\n  var2 = colnames(cor_matrix)[high_corr_indices[, 2]],\n  correlation = cor_matrix[high_corr_indices]\n)\n\n#response correlations\ncor_cpue &lt;- cor_matrix[\"cpue\", ]\n#which are the largest one\nsort(abs(cor_cpue), decreasing = TRUE)\n\n               cpue            latitude                haul surface_temperature \n         1.00000000          0.48166915          0.42204061          0.36076845 \n          longitude                  id  bottom_temperature        bottom_depth \n         0.32925130          0.27546297          0.21331928          0.02460697 \n\n\nI want to use bottom_temperature, bottom_depth, surface_temperature as predictors for the method of co-kriging later as this method requires strongly correlated variables."
  },
  {
    "objectID": "projects/02-project.html#histograms",
    "href": "projects/02-project.html#histograms",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Histograms",
    "text": "Histograms\n\npar(mfrow = c(2,2))\n#histograms\nhist(data_2010$cpue, \n     main = \"Histogram of Catch Number per Area\", \n     xlab = \"CPUE\")\n\nhist(data_2010$bottom_temperature, \n     main = \"Histogram of Bottom Temperature\", \n     xlab = \"bottom_temperature\")\n\nhist(data_2010$bottom_depth, \n     main = \"Histogram of Bottom Depth (m)\", \n     xlab = \"Bottom Depth (m)\")\n\nhist(data_2010$surface_temperature, \n     main = \"Histogram of Surface Temperature\", \n     xlab = \"Surface Temperature\")\n\n\n\n\n\n\n\n\nLooks pretty normal except for the CPUE, this one is skewed right. Let’s test normality for all though."
  },
  {
    "objectID": "projects/02-project.html#tests-for-normality",
    "href": "projects/02-project.html#tests-for-normality",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Tests for Normality",
    "text": "Tests for Normality\n\nshapiro.test(data_2010$cpue)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_2010$cpue\nW = 0.43008, p-value &lt; 2.2e-16\n\nshapiro.test(data_2010$bottom_temperature)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_2010$bottom_temperature\nW = 0.92708, p-value = 1.051e-08\n\nshapiro.test(data_2010$bottom_depth)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_2010$bottom_depth\nW = 0.95349, p-value = 2.487e-06\n\nshapiro.test(data_2010$surface_temperature)\n\n\n    Shapiro-Wilk normality test\n\ndata:  data_2010$surface_temperature\nW = 0.94799, p-value = 7.044e-07\n\n\nSince the p-value is less than 0.05 for all of them, none of them are normal. I should probably log transform the variables later when doing analysis."
  },
  {
    "objectID": "projects/02-project.html#h-scatterplot",
    "href": "projects/02-project.html#h-scatterplot",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "h-scatterplot",
    "text": "h-scatterplot\n\n#hscatterplot\n# Produces h-scatterplots, where point pairs having specific separation distances are plotted\n# first argument is the formula / variable used\n# second is the data set\n# third is the breaks in the data we are detecting in this case 0 to 1 by 0.1\ncoordinates(data_2010) &lt;- ~longitude+latitude\ndist_mat &lt;- dist(coordinates(data_2010))\n#max distance\ndist_max &lt;- max(dist_mat)\nqq &lt;- hscat(cpue~1, data_2010, seq(0, 20, by = 2))\nplot(qq, main=\"h-scatterplots\")\n\n\n\n\n\n\n\n\nThis is a lagged scatterplot matrix that shows the relationship between “cpue” (catch per unit effort) at different distance lags. Each panel represents a scatterplot of cpue values at different lag intervals, and the correlation coefficient is shown for each panel. Since the first panel has the strongest correlation of 0.238 we can conclude that cpue values at nearby locations tend to be somewhat similar and as distance increases this correlation decreases."
  },
  {
    "objectID": "projects/02-project.html#bubble-plot",
    "href": "projects/02-project.html#bubble-plot",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Bubble Plot",
    "text": "Bubble Plot\n\n#bubble plot\nresponse &lt;- as.data.frame(cbind(coordinates(data_2010), data_2010$cpue))\nbubble &lt;- as.geodata(response)\npoints(bubble)\n\n\n\n\n\n\n\n\nThis bubble plot shows the locations of all the observations and their outcome variable magnitudes. The bigger the circle the bigger the cpue variable."
  },
  {
    "objectID": "projects/02-project.html#cross-validation",
    "href": "projects/02-project.html#cross-validation",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Cross Validation",
    "text": "Cross Validation\n\n#ordinary kriging\ncv_ok &lt;- krige.cv(log(cpue)~1, data_2010, model=s.fit2, \n                  nfold=nrow(data_2010)) \npress_ok &lt;- sum(cv_ok$residual^2) / nrow(data_2010)\n\n#universal cv\ncv_uk &lt;- krige.cv(log(cpue)~longitude+latitude, data_2010, model=s.fit2, \n                  nfold=nrow(data_2010)) \npress_uk &lt;- sum(cv_ok$residual^2) / nrow(data_2010)\n\n#cokriging cv\ncv_ck &lt;- gstat.cv(vm_fit, verbose = FALSE)\npress_ck &lt;- sum(cv_ck$residual^2) / nrow(data_2010)\n\n\nc(press_ok, press_uk, press_ck)\n\n[1] 1.417918 1.417918 1.282675\n\n\nCo-kriging is the best at minimizing prediction squared error out of the three we tested so we will make predictions using it."
  },
  {
    "objectID": "projects/02-project.html#predictions-for-co-kriging",
    "href": "projects/02-project.html#predictions-for-co-kriging",
    "title": "Geospatial Predictions on Best Snow Crab Harvesting Grounds",
    "section": "Predictions for Co-Kriging",
    "text": "Predictions for Co-Kriging\n\n#Cokriging predictions:\nck &lt;- predict(vm_fit, grid, verbose = FALSE)\n\n#collapse\npred_mat &lt;- matrix(ck$log_cpue.pred, length(x_val), length(y_val))\n\nWe leave the predictions in matrix form then plot it."
  }
]