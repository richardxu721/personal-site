[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Richard Xu",
    "section": "",
    "text": "Richard Xu is a senior at the University of California, Los Angeles, pursuing a double major in Statistics & Data Science and Economics. Outside of academics and organizing club activities, he loves exploring the underwater world through scuba diving and capturing intricate details of nature through macro photography."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Richard Xu",
    "section": "Education",
    "text": "Education\nUniversity of California, Los Angeles | Los Angeles, CA\n\n\n\nDegree\nDates\n\n\n\n\nB.S. in Statistics & Data Science\nSept 2021 - June 2025\n\n\nB.A. in Economics\nSept 2021 - June 2025"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Richard Xu",
    "section": "Experience",
    "text": "Experience\nNikira Labs | Sales & Marketing Intern | July 2023 - Present"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Richard Xu",
    "section": "",
    "text": "Richard Xu is a senior at the University of California, Los Angeles, pursuing a double major in Statistics & Data Science and Economics. Outside of academics and organizing club activities, he loves exploring the underwater world through scuba diving and capturing intricate details of nature through macro photography."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Title\n\n\n\n\n\n\nTF-IDF on Trump Tweets\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#university-of-california-los-angeles-los-angeles-ca",
    "href": "index.html#university-of-california-los-angeles-los-angeles-ca",
    "title": "Richard Xu",
    "section": "University of California, Los Angeles | Los Angeles, CA",
    "text": "University of California, Los Angeles | Los Angeles, CA"
  },
  {
    "objectID": "index.html#b.s.-in-statistics-data-science-sept-2021---june-2025",
    "href": "index.html#b.s.-in-statistics-data-science-sept-2021---june-2025",
    "title": "Richard Xu",
    "section": "B.S. in Statistics & Data Science | Sept 2021 - June 2025",
    "text": "B.S. in Statistics & Data Science | Sept 2021 - June 2025\nB.A. in Economics | Sept 2021 - June 2025"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-1",
    "href": "projects.html#project-1",
    "title": "",
    "section": "",
    "text": "Here is my project 1"
  },
  {
    "objectID": "projects/01-project.html",
    "href": "projects/01-project.html",
    "title": "TF-IDF on Trump Tweets",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n#nltk.download('stopwords')\npre_2021 = pd.read_csv('tweets_01-08-2021.csv')\n#print(pre_2021.text)\n\n\ndef word_counter(column):\n    dict = {}\n    stop_words = set(stopwords.words('english'))\n    additional_stopwords = {'t.co', 'https', 'http', 'rt', 'amp', 'www', 'co', 'com'}\n    stop_words.update(additional_stopwords)\n    for tweets in column:\n        tweets = tweets.translate(str.maketrans('', '', string.punctuation))\n        separated_words = tweets.split()\n        for item in separated_words:\n            word = item.strip().lower()\n            #word = word.translate(str.maketrans('', '', string.punctuation))\n            if word and word not in stop_words:\n                if word not in dict:\n                    dict[word] = 1\n                else:\n                    dict[word] += 1\n    return dict\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstop_words = set(stopwords.words('english'))\nadditional_stopwords = {'t.co', 'https', 'http', 'rt', 'amp', 'www', 'co', 'com'}\nstop_words.update(additional_stopwords)\nstop_words_list = list(stop_words)\n\n#many different arguments to check out\n#tfidf = TfidfVectorizer(stop_words=stop_words_list, max_features=20, max_df=0.9, min_df = 0.1)\ntfidf = TfidfVectorizer(stop_words=stop_words_list, max_features = 20)\n\nresult = tfidf.fit_transform(pre_2021.text)\n\n# get indexing\nprint('\\nWord indexes:')\nprint(tfidf.vocabulary_)\n \n# display tf-idf values\nprint('\\ntf-idf value:')\nprint(result)\n\n\nWord indexes:\n{'great': np.int64(6), 'country': np.int64(2), 'america': np.int64(0), 'thank': np.int64(15), 'president': np.int64(13), 'realdonaldtrump': np.int64(14), 'trump': np.int64(18), 'get': np.int64(4), 'time': np.int64(17), 'obama': np.int64(10), 'good': np.int64(5), 'people': np.int64(12), 'big': np.int64(1), 'one': np.int64(11), 'never': np.int64(8), 'new': np.int64(9), 'would': np.int64(19), 'donald': np.int64(3), 'like': np.int64(7), 'thanks': np.int64(16)}\n\ntf-idf value:\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 65434 stored elements and shape (56571, 20)&gt;\n  Coords    Values\n  (1, 6)    0.40129118077677894\n  (1, 2)    0.5459962191357245\n  (1, 0)    0.5470521972290333\n  (1, 15)   0.49151542236959284\n  (5, 13)   0.8015308840585026\n  (5, 14)   0.5979533776979568\n  (12, 14)  1.0\n  (13, 14)  1.0\n  (14, 15)  1.0\n  (16, 13)  0.7453165294603974\n  (16, 18)  0.6667107850583405\n  (17, 14)  1.0\n  (20, 4)   1.0\n  (22, 18)  0.45489983229045383\n  (22, 17)  0.6281950757265913\n  (22, 10)  0.6312187334157466\n  (23, 15)  1.0\n  (25, 18)  1.0\n  (28, 17)  0.699888121545114\n  (28, 5)   0.714252488494126\n  (29, 10)  0.7522176224335578\n  (29, 12)  0.6589147505560984\n  (33, 2)   1.0\n  (35, 14)  1.0\n  (36, 1)   1.0\n  : :\n  (56552, 14)   0.5979533776979568\n  (56553, 8)    1.0\n  (56555, 8)    1.0\n  (56556, 13)   0.8015308840585026\n  (56556, 14)   0.5979533776979568\n  (56557, 14)   1.0\n  (56558, 13)   0.8067530039569505\n  (56558, 14)   0.30092457650628834\n  (56558, 5)    0.508521376011813\n  (56559, 15)   1.0\n  (56560, 6)    0.5755836495069655\n  (56560, 11)   0.8177429072882518\n  (56561, 13)   0.41770577163759737\n  (56561, 18)   0.747303815020783\n  (56561, 11)   0.5167774147503184\n  (56562, 18)   1.0\n  (56564, 15)   0.49150041286094875\n  (56564, 18)   0.4122098536023106\n  (56564, 5)    0.5809252579715878\n  (56564, 12)   0.5010351538578134\n  (56566, 7)    1.0\n  (56567, 13)   0.6042607164820945\n  (56567, 14)   0.4507870421675274\n  (56567, 12)   0.6570083935007205\n  (56569, 10)   1.0\n\n\n\n# Convert the sparse matrix to a dense format\ndense_matrix = result.toarray()\n\n# List to store all (index, score) pairs across all documents\nall_scores = []\n\n# Iterate over each document and each word's TF-IDF score\nfor doc_idx, doc_tfidf in enumerate(dense_matrix):\n    for word_idx, score in enumerate(doc_tfidf):\n        if score &gt; 0:  # Only consider non-zero TF-IDF scores\n            all_scores.append((word_idx, score))\n\n# Sort all scores in descending order\nall_scores_sorted = sorted(all_scores, key=lambda x: x[1], reverse=True)\n\n# Get the top N highest TF-IDF scores (let's say top 5)\ntop_n = 5\ntop_scores = all_scores_sorted[:top_n]\n\n# Print the top N TF-IDF scores with corresponding words\nfor idx, score in top_scores:\n    word = list(tfidf.vocabulary_.keys())[list(tfidf.vocabulary_.values()).index(idx)]  # Get word from index\n    print(f\"Word: '{word}', TF-IDF score: {score}\")\n\nWord: 'realdonaldtrump', TF-IDF score: 1.0\nWord: 'realdonaldtrump', TF-IDF score: 1.0\nWord: 'thank', TF-IDF score: 1.0\nWord: 'realdonaldtrump', TF-IDF score: 1.0\nWord: 'get', TF-IDF score: 1.0\n\n\n\nflagged = pre_2021[pre_2021[\"isFlagged\"] != \"f\"]\n\n#tfidf \n#flagged_tfidf = TfidfVectorizer(stop_words='english', max_features=30)\nflagged_tfidf = TfidfVectorizer(stop_words=stop_words_list, max_features=30, ngram_range = (1,3))\nflagged_result = flagged_tfidf.fit_transform(flagged.text)\n\n# get indexing\nprint('\\nWord indexes:')\nprint(flagged_tfidf.vocabulary_.keys())\nprint(\"\\n\")\nprint(flagged_tfidf.vocabulary_)\n \n# display tf-idf values\nprint('\\ntf-idf value:')\nprint(flagged_result)\n\n\nWord indexes:\ndict_keys(['fake', 'votes', 'pennsylvania', 'ballot', 'biden', 'states', 'great', 'thousands', 'election', 'voter', 'fraud', 'dominion', 'rigged', 'georgia', 'poll', 'allowed', 'voting', 'rigged election', 'massive', 'many', '000', 'people', 'ballots', 'vote', 'state', 'big', 'trump', 'fraudulent', 'mail', 'win'])\n\n\n{'fake': np.int64(8), 'votes': np.int64(27), 'pennsylvania': np.int64(16), 'ballot': np.int64(2), 'biden': np.int64(4), 'states': np.int64(22), 'great': np.int64(12), 'thousands': np.int64(23), 'election': np.int64(7), 'voter': np.int64(26), 'fraud': np.int64(9), 'dominion': np.int64(6), 'rigged': np.int64(19), 'georgia': np.int64(11), 'poll': np.int64(18), 'allowed': np.int64(1), 'voting': np.int64(28), 'rigged election': np.int64(20), 'massive': np.int64(15), 'many': np.int64(14), '000': np.int64(0), 'people': np.int64(17), 'ballots': np.int64(3), 'vote': np.int64(25), 'state': np.int64(21), 'big': np.int64(5), 'trump': np.int64(24), 'fraudulent': np.int64(10), 'mail': np.int64(13), 'win': np.int64(29)}\n\ntf-idf value:\n&lt;Compressed Sparse Row sparse matrix of dtype 'float64'\n    with 794 stored elements and shape (304, 30)&gt;\n  Coords    Values\n  (0, 8)    0.7821019410499063\n  (0, 27)   0.6231505065439399\n  (1, 16)   0.6816518223275256\n  (1, 2)    0.7316766998596875\n  (3, 4)    0.7230931896078625\n  (3, 22)   0.6907504897882649\n  (4, 12)   0.9015668151241151\n  (4, 23)   0.4326398939845468\n  (6, 7)    0.3964349712048285\n  (6, 26)   0.7216549371769495\n  (6, 9)    0.567497546473956\n  (7, 8)    1.0\n  (8, 7)    0.4041879731983993\n  (8, 6)    0.695770627820202\n  (8, 19)   0.5937468448627343\n  (9, 8)    0.7345087179053181\n  (9, 11)   0.6785992508992961\n  (13, 7)   0.3820430248395061\n  (13, 19)  0.28060810271676134\n  (13, 18)  0.3331897392323573\n  (13, 1)   0.34261497574795285\n  (13, 28)  0.6663794784647146\n  (13, 20)  0.32068705400402187\n  (14, 9)   1.0\n  (15, 7)   0.6675463883743624\n  : :\n  (291, 3)  1.0\n  (292, 27) 0.41266758763648825\n  (292, 16) 0.48870362611053575\n  (292, 7)  0.3047329966311275\n  (292, 26) 0.5547241981988429\n  (292, 9)  0.4362259651116138\n  (294, 4)  0.4460855074916803\n  (294, 22) 0.42613287915825565\n  (294, 7)  0.5314335408658261\n  (294, 17) 0.3836064731293578\n  (294, 5)  0.43570971382590395\n  (295, 2)  0.6975466992764388\n  (295, 13) 0.7165393236442404\n  (300, 7)  0.41010048759879475\n  (300, 15) 0.7059484513308978\n  (300, 3)  0.5774550840842407\n  (301, 22) 0.5752702728457332\n  (301, 14) 0.643383495882818\n  (301, 3)  0.5050958229936177\n  (302, 7)  1.0\n  (303, 16) 0.37217827638835577\n  (303, 9)  0.3322132661531879\n  (303, 3)  0.6535560820231491\n  (303, 24) 0.39443571249237325\n  (303, 13) 0.41036884961898396"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "",
    "section": "",
    "text": "#look at notes from jose’s repo\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\n\nNo matching items"
  }
]