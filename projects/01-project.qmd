---
title: TF-IDF on Trump Tweets
jupyter: python3
---

```{python}
import pandas as pd
import numpy as np
import string
import nltk
from nltk.corpus import stopwords
#nltk.download('stopwords')
pre_2021 = pd.read_csv('tweets_01-08-2021.csv')
#print(pre_2021.text)
```

```{python}
def word_counter(column):
    dict = {}
    stop_words = set(stopwords.words('english'))
    additional_stopwords = {'t.co', 'https', 'http', 'rt', 'amp', 'www', 'co', 'com'}
    stop_words.update(additional_stopwords)
    for tweets in column:
        tweets = tweets.translate(str.maketrans('', '', string.punctuation))
        separated_words = tweets.split()
        for item in separated_words:
            word = item.strip().lower()
            #word = word.translate(str.maketrans('', '', string.punctuation))
            if word and word not in stop_words:
                if word not in dict:
                    dict[word] = 1
                else:
                    dict[word] += 1
    return dict
```

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer

stop_words = set(stopwords.words('english'))
additional_stopwords = {'t.co', 'https', 'http', 'rt', 'amp', 'www', 'co', 'com'}
stop_words.update(additional_stopwords)
stop_words_list = list(stop_words)

#many different arguments to check out
#tfidf = TfidfVectorizer(stop_words=stop_words_list, max_features=20, max_df=0.9, min_df = 0.1)
tfidf = TfidfVectorizer(stop_words=stop_words_list, max_features = 20)

result = tfidf.fit_transform(pre_2021.text)

# get indexing
print('\nWord indexes:')
print(tfidf.vocabulary_)
 
# display tf-idf values
print('\ntf-idf value:')
print(result)
```

```{python}
# Convert the sparse matrix to a dense format
dense_matrix = result.toarray()

# List to store all (index, score) pairs across all documents
all_scores = []

# Iterate over each document and each word's TF-IDF score
for doc_idx, doc_tfidf in enumerate(dense_matrix):
    for word_idx, score in enumerate(doc_tfidf):
        if score > 0:  # Only consider non-zero TF-IDF scores
            all_scores.append((word_idx, score))

# Sort all scores in descending order
all_scores_sorted = sorted(all_scores, key=lambda x: x[1], reverse=True)

# Get the top N highest TF-IDF scores (let's say top 5)
top_n = 5
top_scores = all_scores_sorted[:top_n]

# Print the top N TF-IDF scores with corresponding words
for idx, score in top_scores:
    word = list(tfidf.vocabulary_.keys())[list(tfidf.vocabulary_.values()).index(idx)]  # Get word from index
    print(f"Word: '{word}', TF-IDF score: {score}")
```

```{python}
flagged = pre_2021[pre_2021["isFlagged"] != "f"]

#tfidf 
#flagged_tfidf = TfidfVectorizer(stop_words='english', max_features=30)
flagged_tfidf = TfidfVectorizer(stop_words=stop_words_list, max_features=30, ngram_range = (1,3))
flagged_result = flagged_tfidf.fit_transform(flagged.text)

# get indexing
print('\nWord indexes:')
print(flagged_tfidf.vocabulary_.keys())
print("\n")
print(flagged_tfidf.vocabulary_)
 
# display tf-idf values
print('\ntf-idf value:')
print(flagged_result)
```

